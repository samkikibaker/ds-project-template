# {{ cookiecutter.project_name }}
{{ cookiecutter.description }}

## Project Structure
```
â”œâ”€â”€ .env                   <- Local config and secrets that should not be stored in source control.
â”œâ”€â”€ Makefile               <- Makefile with useful commands for project setup and running analysis.
â”œâ”€â”€ README.md              <- The top-level README for developers using this project.
â”œâ”€â”€ app                    <- App-specific code, requirements file and Dockerfile.
â”œâ”€â”€ assets                 <- Assets for use in web-apps.
â”œâ”€â”€ azureml                <- Scripts for creating Azure ML assets and running jobs.
â”œâ”€â”€ conf                   <- Configuration files that can be stored in source control.
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ 01_raw             <- The original, immutable data dump.
â”‚   â”œâ”€â”€ 02_intermediate    <- Intermediate data that has been transformed.
â”‚   â”œâ”€â”€ 03_model_input     <- The final, canonical data sets for modeling.
â”‚   â””â”€â”€ 04_model_output    <- Outputs from models (e.g. predictions).
â”œâ”€â”€ models                 <- Trained and serialized models or model summaries.
â”œâ”€â”€ notebooks              <- Jupyter notebooks.
â”œâ”€â”€ pyproject.toml         <- Project metadata and dependencies.
â”œâ”€â”€ .python-version        <- Specifies the version of Python to use
â”œâ”€â”€ references             <- Data dictionaries, manuals, and all other explanatory materials.
â”œâ”€â”€ src                    <- Source code for use in this project.
â”‚   â””â”€â”€ {{ cookiecutter.package_name }}
â”‚       â”œâ”€â”€ __init__.py    <- Make {{ cookiecutter.package_name }} a Python module.
â”‚       â”œâ”€â”€ data           <- Scripts to download or generate data.
â”‚       â”œâ”€â”€ features       <- Scripts to turn raw data into features for modeling.
â”‚       â”œâ”€â”€ model          <- Scripts to train models and make predictions.
â”‚       â”œâ”€â”€ utils          <- Utility functions.
â”‚       â””â”€â”€ visualization  <- Scripts to create exploratory and results-oriented visualizations.
â””â”€â”€ tests                  <- Tests for functions in src.
```

## Getting Started

### Setup

1. **Create virtual environment and install dependencies using uv**:
   ```bash
   pip install uv
   uv sync
   pre-commit install
   ```

   Alternatively, you can use the `make` command:
   ```bash
   make install
   ```

3. **Make Initial Commit**:
   ```bash
   git add .
   git commit -m "Initial commit"
   ```

4. **Update Environment Variables**:
Much of the codebase is reliant on environment variables. Set any relevant variables in the `.env` file.
**Please Note: You can define secrets in `.env` but note that any variables passed in the definition of an Azure ML command job will be exposed. In this case, use Key Vault.**

### Usage

#### Azure ML
- **Component Registration**: There is boilerplate code in `azureml` for creating and registering data assets and environments.
- **Jobs and Pipelines**: Azure ML Jobs should be defined and run from the `azureml` directory. These can be orchestrated using the Azure ML Pipelines service.
- **Utility Functions**: There are a number of utility functions for interacting with Azure ML in the `{{cookiecutter.package_name}}.utils.azure_ml` module. 

#### Data

- **Immutability**: Raw data should not be edited. Transform data through your processing pipeline.
- **Local Directory Structure**: Organize any local data into `01_raw`, `02_intermediate`, `03_model_input`, and `04_model_output`.
- **Cloud Data Storage**: If using Azure ML you should upload data to Datastores. To create a local copy of any registered data assets use the `{{cookiecutter.package_name}}.utils.azure_ml.sync_data_local` function.

#### Code Quality

-- **Ruff**: Ruff provides fast linting and auto-fixing for Python code, combining functionality of tools like Flake8 and Black for streamlined code quality.
- **pytest**: The pytest framework makes it easy to write small, readable tests, and can scale to support complex functional testing for applications and libraries.

#### Notebooks

- **Purpose**: Notebooks are for exploration and communication. Refactor useful code into the `src` directory.
- **nbstripout**: Notebook output should ~not~ rarely be committed to source control because it creates ugly diffs and risks data leakage. Nbstripout is installed as a pre-commit hook. It can be ignored by setting the ```"keep_output": true``` metadata on a cell.
- **Auto-reloading**:
  ```pythonðŸš¡
  %load_ext autoreload
  %autoreload 2
  ```

#### Applications

- **Streamlit and FastAPI**: If selected, templates are provided with `requirements.txt` and `Dockerfile` for building containerized apps.

### Database Connections

- **Database Connections**: If selected, utility functions for database connections are in `utils/db.py`. Configuration settings are added to `.env`.

### Project Philosophy

The goal is to maintain modularity and separation of concerns:
- **Shared Code**: All reusable code should reside in the `src/{{ cookiecutter.package_name }}` directory.
- **Apps, Pipelines, and Notebooks**: Use the shared code in apps, pipelines, and notebooks, ensuring that your project remains clean and maintainable.